{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b90b206-3da1-48de-abcb-ac77e242412d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final save\n",
      "Scraping complete. Last_scraped_row: 77742\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "# Function to save scraped data to a CSV file\n",
    "def save_data(scraped_data_list, filename='cochesdotcom_detail_pages_2024-05-13.csv'):\n",
    "    if scraped_data_list:\n",
    "        # Convert list of dictionaries to DataFrame\n",
    "        df_to_append = pd.DataFrame(scraped_data_list)\n",
    "        # Check if file exists to append; if not, write a new file\n",
    "        mode = 'a' if os.path.exists(filename) else 'w'\n",
    "        header = not os.path.exists(filename) or mode == 'w'\n",
    "        # Write DataFrame to CSV file\n",
    "        df_to_append.to_csv(filename, index=False, mode=mode, header=header)\n",
    "        print(f\"Data saved to {filename}.\")\n",
    "        # Save the last scraped row number to a text file\n",
    "        with open('last_scraped_row.txt', 'w') as file:\n",
    "            file.write(str(last_scraped_row))\n",
    "\n",
    "# Function to scrape data from detail_page URL\n",
    "def scrape_detail_page(url):\n",
    "    try:\n",
    "        # Set up HTTP request with user-agent header\n",
    "        headers = {'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:60.0) Gecko/20100101 Firefox/60.0'}\n",
    "        res = requests.get(url, headers=headers)\n",
    "        # Parse the HTML content of the page\n",
    "        soup = BeautifulSoup(res.text, 'lxml')\n",
    "\n",
    "        # Initialize dictionary to store scraped data\n",
    "        scraped_data = {}\n",
    "\n",
    "        # Extract make and model from breadcrumbs navigation\n",
    "        make_and_model = soup.find('nav', class_='Breadcrumbs-module_breadcrumbs__ktfbh font-body-s')\n",
    "        if make_and_model:\n",
    "            attributes = make_and_model.find_all(class_='TextButton-module_content__eBnG1')\n",
    "            if len(attributes) >= 2:\n",
    "                make = attributes[-2].text.strip()\n",
    "                model = attributes[-1].text.strip()\n",
    "                scraped_data['make'] = make\n",
    "                scraped_data['model'] = model\n",
    "\n",
    "        # Extract cash and financing prices\n",
    "        list = soup.find('div', class_='ClassifiedPriceBlock_classified-price-block__SnJtL Container-module_container__JMoiT Container-module_is-full-width__oqQVQ')\n",
    "        if list:\n",
    "            prices = list.find_all('strong', class_='PriceLabel_copy__4E1Gm font-heading-m font-bold')\n",
    "            if prices:\n",
    "                cash_price_text = prices[0].text.strip()\n",
    "                cash_price_numeric = re.search(r'([\\d\\.]+)', cash_price_text).group(1) if cash_price_text else None\n",
    "                financing_price_text = prices[1].text.strip() if len(prices) > 1 else \"\"\n",
    "                financing_price_numeric = re.search(r'([\\d\\.]+)', financing_price_text).group(1) if financing_price_text else None\n",
    "                scraped_data['cash'] = cash_price_numeric\n",
    "                scraped_data['fin'] = financing_price_numeric\n",
    "\n",
    "        # Extract various car features and specifications\n",
    "        attribute_list = soup.find('ul', class_='Grid-module_grid__h49fk CarOverview-module_car-overview__ysD5f CarOverview-module_background-color-neutral-5__GrRlH CarOverview-module_has-padding__N9hjr CarProfileCarDetails_car-over-view-details__DywRB')\n",
    "        if attribute_list:\n",
    "            attributes = attribute_list.find_all('li', class_='Grid-module_grid__h49fk')\n",
    "            for attribute in attributes:\n",
    "                span = attribute.find('p', class_='FeatureText-module_feature-text__BpFHF font-body-m FeatureText-module_align-left__bm1gj FeatureText-module_vertical-align-center__kK9Mq color-inherit FeatureText-module_is-center-on-mobile__HHn4A font-bold FeatureText-module_has-icon-on-mobile__3bhQE CarOverview-module_feature-text__DHOtg').find('span', class_='FeatureText-module_wrapper__nGlYu')\n",
    "                title = span.find('small', class_='FeatureText-module_eyebrow__UCQaZ font-regular font-body-xs color-inherit FeatureText-module_is-eyebrow-upper-case__L8oNT').text.strip()\n",
    "                value = span.find('span').text.strip()\n",
    "                scraped_data[title] = value\n",
    "\n",
    "        # Extract detailed technical data from tables\n",
    "        tables = soup.find_all('table', class_='SpecsDetailTable-module_table__G60yw')\n",
    "        if tables:\n",
    "            for table in tables:\n",
    "                specs = table.find_all('tr', class_='SpecsDetailTableRow-module_tr__MbBui')\n",
    "                if specs:\n",
    "                    for spec in specs:\n",
    "                        cells = spec.find_all('td', class_='SpecsDetailTableCell-module_td__b-Z4y font-body-s')\n",
    "                        if len(cells) == 2:\n",
    "                            title = cells[0].text.strip()\n",
    "                            value = cells[1].text.strip()\n",
    "                            scraped_data[title] = value\n",
    "                \n",
    "        return scraped_data\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping URL {url}: {str(e)}\")\n",
    "        return None\n",
    "        \n",
    "try:\n",
    "    df_urls = pd.read_csv('cochesdotcom_urls.csv')\n",
    "    try:\n",
    "        with open('last_scraped_row.txt', 'r') as file:\n",
    "            last_scraped_row = int(file.read().strip())\n",
    "    except FileNotFoundError:\n",
    "        last_scraped_row = 0\n",
    "\n",
    "    scraped_data_list = []\n",
    "\n",
    "    for i, url in enumerate(df_urls['url'][last_scraped_row:]):\n",
    "        if pd.isna(url):\n",
    "            continue\n",
    "        i += last_scraped_row  # Adjust the index to the correct row number\n",
    "        if i >= last_scraped_row + 30000:\n",
    "            break\n",
    "\n",
    "        print(f\"Scraping URL: {url}\")\n",
    "        scraped_data = scrape_detail_page(url)\n",
    "        last_scraped_row += 1\n",
    "        print(last_scraped_row)\n",
    "        if scraped_data:\n",
    "            scraped_data['Date'] = datetime.now().strftime('%Y-%m-%d')\n",
    "            scraped_data['URL'] = url\n",
    "            scraped_data_list.append(scraped_data)\n",
    "\n",
    "        # Checkpoint every 100 URLs\n",
    "        if (i - last_scraped_row) % 500 == 0 and scraped_data_list:\n",
    "            save_data(scraped_data_list)\n",
    "            scraped_data_list.clear()  # Clear the list after saving\n",
    "            print (\"500 rows saved\")\n",
    "\n",
    "    save_data(scraped_data_list)  # Final save\n",
    "    print (\"final save\")\n",
    "\n",
    "    with open('last_scraped_row.txt', 'w') as file:\n",
    "        file.write(str(last_scraped_row))\n",
    "    print(f\"Scraping complete. Last_scraped_row: {last_scraped_row}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {str(e)}\")\n",
    "    save_data(scraped_data_list)  # Save any remaining data before exiting\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"KeyboardInterrupt error occurred\")\n",
    "    save_data(scraped_data_list)  # Save any remaining data before exiting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba3a730-acae-4a67-947f-83efeb5b0867",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
