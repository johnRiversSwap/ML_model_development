{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f21ea066-ed75-4801-a6e2-3f9aa32644d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preparation and Defining X and y\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import category_encoders as ce\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "df = pd.read_csv('clean_cochesdotcom_detail_pages_2024-05-13.csv')\n",
    "\n",
    "# Convert 'Año' to datetime, specifying the format to correctly parse month and year\n",
    "df['Año'] = pd.to_datetime(df['Año'], format='%m/%Y')\n",
    "\n",
    "# Calculate 'Age' in days from 'Año' to the current date\n",
    "df['Age'] = (datetime.now() - df['Año']).dt.days\n",
    "\n",
    "# Drop the 'Año' column after calculating 'Age'\n",
    "df = df.drop(['Año'], axis=1)\n",
    "\n",
    "# Imputing missing values based on 'make', 'model', 'Cambio', 'Potencia'\n",
    "for column in df.columns:\n",
    "    if df[column].dtype == 'float64' or df[column].dtype == 'int64':\n",
    "        # Impute numeric columns with the mean of the group\n",
    "        df[column] = df.groupby(['make', 'model', 'Cambio', 'Potencia (cv)'])[column].transform(lambda x: x.fillna(x.mean()))\n",
    "    elif df[column].dtype == 'bool':\n",
    "        # Impute boolean columns with the mode of the group, or \"Unknown\" if mode is not available\n",
    "        df[column] = df.groupby(['make', 'model', 'Cambio', 'Potencia (cv)'])[column].transform(lambda x: x.fillna(x.mode().iloc[0] if not x.mode().empty else \"Unknown\"))\n",
    "\n",
    "# Drop rows that still have missing values after the imputation\n",
    "df = df.dropna()\n",
    "\n",
    "# One-hot encoding, filling NaNs, etc. (Ensure 'cash' is not affected)\n",
    "categorical_cols = ['Combustible', 'Cambio', 'Vendedor', 'Transmisión', 'Tracción', 'Carrocería']\n",
    "df = pd.get_dummies(df, columns=categorical_cols, drop_first=True)\n",
    "\n",
    "target = 'cash'\n",
    "high_cardinality_features = ['make', 'model', 'Color']\n",
    "encoder = ce.TargetEncoder(cols=high_cardinality_features)\n",
    "df_encoded = encoder.fit_transform(df[high_cardinality_features], df[target])\n",
    "df.drop(high_cardinality_features, axis=1, inplace=True)\n",
    "df = pd.concat([df, df_encoded], axis=1)\n",
    "\n",
    "# Preparing features and target\n",
    "X = df.drop('cash', axis=1)\n",
    "y = df['cash']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a8661f0-4c62-485d-9cb7-6e0ce491b910",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numerical columns: ['Kms (km)', 'Potencia (cv)', 'Maletero (L)', 'Anchura (cm)', 'Altura (cm)', 'Longitud (cm)', 'Puertas (puertas)', 'Plazas (plazas)', 'Depósito (L)', 'Peso máximo (kg)', 'Velocidad máxima (km/h)', 'Consumo mixto (L)', 'Consumo urbano (L)', 'Consumo extraurbano (L)', '0-100 km/h (s)', 'Autonomía', 'Emisiones de CO2 (gr/m3)', 'Cilindrada (cm3)', 'Marchas', 'Par máximo (nm)', 'Age', 'make', 'model', 'Color']\n"
     ]
    }
   ],
   "source": [
    "#Define Full Preprocessor with PCA and Standardization\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Pipeline setup\n",
    "numerical_cols = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "print(\"Numerical columns:\", numerical_cols)\n",
    "\n",
    "# Define transformers and pipeline\n",
    "numerical_transformer = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('pca', PCA(n_components=0.95))\n",
    "])\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('num', numerical_transformer, numerical_cols)\n",
    "], remainder='passthrough')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059879ff-1eaf-46df-b821-bb41c610f295",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define Preprocessor Without PCA --> Skipping this one since the top performer is with PCA and StandardScaler (same for rest of prep.)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Pipeline setup\n",
    "numerical_cols = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "print(\"Numerical columns:\", numerical_cols)\n",
    "\n",
    "#Define Preprocessor Without PCA\n",
    "preprocessor_no_pca = ColumnTransformer(transformers=[\n",
    "    ('num', StandardScaler(), numerical_cols)\n",
    "], remainder='passthrough')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adac0929-6a12-4591-8c43-f07eef4555f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define preprocessor without Standardization\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Pipeline setup\n",
    "numerical_cols = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "print(\"Numerical columns:\", numerical_cols)\n",
    "\n",
    "# Define preprocessor without Standardization\n",
    "preprocessor_no_standardization = ColumnTransformer(transformers=[\n",
    "    ('num', PCA(n_components=0.95), numerical_cols)\n",
    "], remainder='passthrough')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f7fe5c-a833-4371-8905-d25a60fa6f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define preprocessor without both PCA and Standardization\n",
    "preprocessor_no_both = 'passthrough'  # This uses no preprocessing on numerical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f2f15b0-de71-451e-ac2e-35f1357e8a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Define a consistent KFold splitter\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "def evaluate_with_cv(preprocessor, X, y, params, cv=kf):\n",
    "    model = RandomForestRegressor(**params)\n",
    "    pipeline = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('model', model)\n",
    "    ])\n",
    "    # Using cross_val_score for consistent evaluation\n",
    "    scores = cross_val_score(pipeline, X, y, cv=cv, scoring='r2')\n",
    "    mse_scores = cross_val_score(pipeline, X, y, cv=cv, scoring='neg_mean_squared_error')\n",
    "    return np.mean(scores), np.mean(-mse_scores)  # Convert MSE to positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32cf587f-ed54-45fe-8082-b03810ba573e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RandomForest parameters\n",
    "rf_params = {'n_estimators': 200, 'min_samples_split': 5, 'max_features': 'sqrt', 'max_depth': 20}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f075be16-e0a2-4aaf-a49a-47a1b309b860",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Preprocessing - R2: 0.7113412536358774, MSE: -113236066.80684654\n"
     ]
    }
   ],
   "source": [
    "r2_full, mse_full = evaluate_with_cv(preprocessor, X, y, rf_params)\n",
    "print(f\"Full Preprocessing - R2: {r2_full}, MSE: {-mse_full}\")  # neg_mean_squared_error is negative\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10fc7f1a-a80b-4ab4-b8cd-cb6a5ce50111",
   "metadata": {},
   "outputs": [],
   "source": [
    "#How would it look for rest of preprocessors\n",
    "r2_no_pca, mse_no_pca = evaluate_with_cv(preprocessor_no_pca, X, y, rf_params)\n",
    "print(f\"No PCA - R2: {r2_no_pca}, MSE: {mse_no_pca}\")\n",
    "r2_no_standardization, mse_no_standardization = evaluate_with_cv(preprocessor_no_standardization, X, y, rf_params)\n",
    "print(f\"No Standardization - R2: {r2_no_standardization}, MSE: {mse_no_standardization}\")\n",
    "r2_no_both, mse_no_both = evaluate_with_cv(preprocessor_no_both, X, y, rf_params)\n",
    "print(f\"No PCA & No Standardization - R2: {r2_no_both}, MSE: {mse_no_both}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf4ec93-da12-4e8d-ba67-75e622f6d0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tests for understanding PCA impact --> ran in ML_cochesdotcom_first.\n",
    "import pandas as pd \n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "df = pd.read_csv('clean_cochesdotcom_detail_pages_2024-05-13.csv')\n",
    "\n",
    "if 'cash' in df.columns:\n",
    "\n",
    "    # Convert 'Año' to datetime, specifying the format to correctly parse month and year\n",
    "    df['Año'] = pd.to_datetime(df['Año'], format='%m/%Y')\n",
    "    \n",
    "    # Calculate 'Age' in days from 'Año' to the current date\n",
    "    df['Age'] = (datetime.now() - df['Año']).dt.days\n",
    "    \n",
    "    # Drop the 'Año' column after calculating 'Age'\n",
    "    df = df.drop(['Año'], axis=1)\n",
    "\n",
    "    # One-hot encoding, filling NaNs, etc. (Ensure 'cash' is not affected)\n",
    "    categorical_cols = ['Combustible', 'Cambio', 'Vendedor', 'Transmisión', 'Tracción', 'Carrocería']\n",
    "    df = pd.get_dummies(df, columns=categorical_cols, drop_first=True)\n",
    "\n",
    "    target = 'cash'\n",
    "    high_cardinality_features = ['make', 'model', 'Color']\n",
    "    encoder = ce.TargetEncoder(cols=high_cardinality_features)\n",
    "    df_encoded = encoder.fit_transform(df[high_cardinality_features], df[target])\n",
    "    df.drop(high_cardinality_features, axis=1, inplace=True)\n",
    "    df = pd.concat([df, df_encoded], axis=1)\n",
    "\n",
    "    # Preparing features and target\n",
    "    X = df.drop('cash', axis=1)\n",
    "    y = df['cash']\n",
    "    \n",
    "    # Assuming 'X' is your feature matrix before applying the full pipeline\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    pca = PCA()\n",
    "    X_pca = pca.fit_transform(X_scaled)\n",
    "    \n",
    "    # Plot the cumulative variance explained by all the components\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    plt.plot(np.cumsum(pca.explained_variance_ratio_), marker='o')\n",
    "    plt.xlabel('Number of Components')\n",
    "    plt.ylabel('Cumulative Explained Variance')\n",
    "    plt.title('Explained Variance by PCA Components')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    # Get the loadings of the first few components\n",
    "    loadings = pca.components_.T * np.sqrt(pca.explained_variance_)\n",
    "    \n",
    "    # Convert loadings into a DataFrame for better readability\n",
    "    loading_matrix = pd.DataFrame(loadings, columns=[f'PC{i+1}' for i in range(len(pca.components_))], index=X.columns)\n",
    "    print(loading_matrix)\n",
    "\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    plt.plot(range(1, len(pca.explained_variance_) + 1), pca.explained_variance_, 'o-')\n",
    "    plt.xlabel('Component Number')\n",
    "    plt.ylabel('Eigenvalue (Explained Variance)')\n",
    "    plt.title('Scree Plot')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1097337-3eea-4bf5-93d6-3c857c04a6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying the different models and comparing performance (Random Forest and XGBoost)\n",
    "import time\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import make_scorer, r2_score, mean_squared_error\n",
    "from sklearn.model_selection import KFold, cross_val_score, cross_validate\n",
    "\n",
    "# Models setup\n",
    "models = {\n",
    "    'RandomForest': RandomForestRegressor(random_state=42),\n",
    "    'XGBoost': XGBRegressor(objective='reg:squarederror', random_state=42)\n",
    "}\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_models(models, X, y):\n",
    "    results = []\n",
    "    scoring = {'MSE': make_scorer(mean_squared_error, greater_is_better=False),\n",
    "               'R2': 'r2'}\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        start_time = time.time()\n",
    "        pipeline = Pipeline([\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('model', model)\n",
    "        ])\n",
    "        scores = cross_validate(pipeline, X, y, cv=kf, scoring=scoring, return_train_score=False)\n",
    "        duration = time.time() - start_time\n",
    "        results.append({\n",
    "            'Model': name,\n",
    "            'MSE': -np.mean(scores['test_MSE']),  # Negate to make positive\n",
    "            'R2': np.mean(scores['test_R2']),\n",
    "            'Training Time (s)': duration\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Run evaluation\n",
    "results_df = evaluate_models(models, X, y)\n",
    "print(results_df)\n",
    "\n",
    "# Recommendation based on MSE and R2\n",
    "best_model = results_df.sort_values(by=['MSE', 'R2'], ascending=[True, False]).iloc[0]\n",
    "print(f\"Recommended model based on MSE and R²: {best_model['Model']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a581e4-c0bd-461e-8faa-a54365a9e43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying the different models and comparing performance (NN)\n",
    "import time\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import make_scorer, r2_score, mean_squared_error\n",
    "from sklearn.model_selection import KFold, cross_val_score, cross_validate\n",
    "\n",
    "# Models setup\n",
    "models = {\n",
    "    'MLPRegressor': MLPRegressor(hidden_layer_sizes=(100, 50), max_iter=1500, alpha=0.001, learning_rate_init=0.005, solver='adam', random_state=42)\n",
    "}\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_models(models, X, y):\n",
    "    results = []\n",
    "    scoring = {'MSE': make_scorer(mean_squared_error, greater_is_better=False),\n",
    "               'R2': 'r2'}\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        start_time = time.time()\n",
    "        pipeline = Pipeline([\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('model', model)\n",
    "        ])\n",
    "        scores = cross_validate(pipeline, X, y, cv=kf, scoring=scoring, return_train_score=False)\n",
    "        duration = time.time() - start_time\n",
    "        results.append({\n",
    "            'Model': name,\n",
    "            'MSE': -np.mean(scores['test_MSE']),  # Negate to make positive\n",
    "            'R2': np.mean(scores['test_R2']),\n",
    "            'Training Time (s)': duration\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Run evaluation\n",
    "results_df = evaluate_models(models, X, y)\n",
    "print(results_df)\n",
    "\n",
    "# Recommendation based on MSE and R2\n",
    "best_model = results_df.sort_values(by=['MSE', 'R2'], ascending=[True, False]).iloc[0]\n",
    "print(f\"Recommended model based on MSE and R²: {best_model['Model']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd63193-fb23-463e-a358-11e597062ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparametritation optimization with Optuna library --> Best is trial 11 with value: 0.4925261281110004.{'hidden_layer_sizes': '100', 'max_iter': 1500, 'alpha': 0.00027907050619925316, 'learning_rate_init': 0.0038619320536359453}\n",
    "\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "import optuna\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming 'X' and 'y' are your features and target data\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "def objective(trial):\n",
    "    hidden_layer_config = trial.suggest_categorical('hidden_layer_sizes', ['50', '100', '50-50', '100-50'])\n",
    "    hidden_layer_sizes = tuple(map(int, hidden_layer_config.split('-'))) if '-' in hidden_layer_config else (int(hidden_layer_config),)\n",
    "\n",
    "    params = {\n",
    "        'hidden_layer_sizes': hidden_layer_sizes,\n",
    "        'max_iter': trial.suggest_categorical('max_iter', [1000, 1500, 2000]),\n",
    "        'alpha': trial.suggest_float('alpha', 1e-4, 1e-2, log=True),\n",
    "        'learning_rate_init': trial.suggest_float('learning_rate_init', 0.001, 0.1, log=True)\n",
    "    }\n",
    "    mlp = MLPRegressor(**params, random_state=42)\n",
    "    mlp.fit(X_train, y_train)\n",
    "    return mlp.score(X_val, y_val)\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=50)\n",
    "print(study.best_params)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8fa23f5-b48a-496c-92cb-c996aaac5be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLPRegressor with different activation functions (default is relu)\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "import optuna\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Assuming 'X' and 'y' are your features and target data already defined\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "def objective(trial):\n",
    "    # Suggesting hidden layer configurations as strings\n",
    "    hidden_layer_config = trial.suggest_categorical('hidden_layer_sizes', ['50', '100', '50-50', '100-50'])\n",
    "    # Converting string representation to tuple of integers\n",
    "    hidden_layer_sizes = tuple(map(int, hidden_layer_config.split('-'))) if '-' in hidden_layer_config else (int(hidden_layer_config),)\n",
    "\n",
    "    params = {\n",
    "        'hidden_layer_sizes': hidden_layer_sizes,\n",
    "        'max_iter': trial.suggest_categorical('max_iter', [1000, 1500, 2000]),\n",
    "        'alpha': trial.suggest_float('alpha', 1e-4, 1e-2, log=True),\n",
    "        'learning_rate_init': trial.suggest_float('learning_rate_init', 0.001, 0.1, log=True),\n",
    "        #excluding default one ('relu')\n",
    "        'activation': trial.suggest_categorical('activation', ['identity', 'logistic', 'tanh'])\n",
    "    }\n",
    "    mlp = MLPRegressor(**params, random_state=42)\n",
    "    mlp.fit(X_train, y_train)\n",
    "    y_pred = mlp.predict(X_val)\n",
    "    score = r2_score(y_val, y_pred)\n",
    "    return score\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=50)\n",
    "print('Best trial:', study.best_trial.value)\n",
    "print('Best parameters:', study.best_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4685584e-3f06-447a-902b-867d1d71aa79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparametrization for RandomForest and XGBoost --> This piece of code is which show me which was the best configuration:           \n",
    "#Model  Best Score (R2)  \\\n",
    "#0  RandomForest         0.699499   \n",
    "#1       XGBoost         0.664087   \n",
    "\n",
    "#                                     Best Parameters  Training Time (s)  \n",
    "#0  {'n_estimators': 200, 'min_samples_split': 5, ...         238.308649  \n",
    "#1  {'subsample': 0.8, 'n_estimators': 300, 'max_d...           7.115866  \n",
    "#Recommended model based on R²: RandomForest with parameters {'n_estimators': 200, 'min_samples_split': 5, 'max_features': 'sqrt', 'max_depth': 20}\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import train_test_split, KFold, RandomizedSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, make_scorer, r2_score\n",
    "import time\n",
    "\n",
    "# Model setup\n",
    "model_params = {\n",
    "    'RandomForest': {\n",
    "        'model': RandomForestRegressor(random_state=42),\n",
    "        'params': {\n",
    "            'n_estimators': [100, 200, 300],\n",
    "            'max_features': ['auto', 'sqrt', 'log2'],\n",
    "            'max_depth': [None, 10, 20, 30],\n",
    "            'min_samples_split': [2, 5, 10]\n",
    "        }\n",
    "    },\n",
    "    'XGBoost': {\n",
    "        'model': XGBRegressor(objective='reg:squarederror', random_state=42),\n",
    "        'params': {\n",
    "            'n_estimators': [100, 200, 300],\n",
    "            'learning_rate': [0.01, 0.05, 0.1],\n",
    "            'max_depth': [3, 5, 7],\n",
    "            'subsample': [0.7, 0.8, 0.9]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "def evaluate_models(model_params, X, y):\n",
    "    results = []\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    \n",
    "    for name, model_info in model_params.items():\n",
    "        start_time = time.time()\n",
    "        model_pipeline = Pipeline([\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('model', RandomizedSearchCV(model_info['model'], model_info['params'], n_iter=10, cv=kf, scoring='r2', n_jobs=-1, random_state=42))\n",
    "        ])\n",
    "        \n",
    "        # Fit and evaluate the pipeline\n",
    "        model_pipeline.fit(X, y)\n",
    "        best_model = model_pipeline.named_steps['model'].best_estimator_\n",
    "        best_score = model_pipeline.named_steps['model'].best_score_\n",
    "        duration = time.time() - start_time\n",
    "        \n",
    "        results.append({\n",
    "            'Model': name,\n",
    "            'Best Score (R2)': best_score,\n",
    "            'Best Parameters': model_pipeline.named_steps['model'].best_params_,\n",
    "            'Training Time (s)': duration\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "    \n",
    "# Assuming 'X' and 'y' are your features and target data already loaded\n",
    "results_df = evaluate_models(model_params, X, y)\n",
    "print(results_df)\n",
    "\n",
    "# To decide which model to use:\n",
    "best_model = results_df.loc[results_df['Best Score (R2)'].idxmax()]\n",
    "print(f\"Recommended model based on R²: {best_model['Model']} with parameters {best_model['Best Parameters']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8db55640-3be4-46d8-98c4-ba7c38871ec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: joblib in c:\\users\\juano\\anaconda3\\lib\\site-packages (1.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "61ea9914-5187-4257-aaf5-a0091b44cf17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline not defined.\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Assuming you have already created and trained 'pipeline' with your RandomForestRegressor\n",
    "# Save the model to a file\n",
    "if 'pipeline' in locals() or 'pipeline' in globals():\n",
    "    joblib.dump(pipeline, 'model_rf.joblib')\n",
    "    print(\"Pipeline saved successfully.\")\n",
    "else:\n",
    "    print(\"Pipeline not defined.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dc22b0f0-ea23-4f4a-bdc3-f8a486014e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating JSON files for website dropdowns based on make and model\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('clean_cochesdotcom_detail_pages_2024-05-13.csv')\n",
    "\n",
    "# Helper function to convert data types for JSON serialization\n",
    "def convert_for_json(item):\n",
    "    if isinstance(item, np.ndarray):\n",
    "        return item.tolist()  # Convert numpy arrays to Python list\n",
    "    elif isinstance(item, (np.int64, np.int32)):\n",
    "        return int(item)  # Convert numpy integers to Python int\n",
    "    elif isinstance(item, (np.float64, np.float32)):\n",
    "        return float(item)  # Convert numpy floats to Python float\n",
    "    return item  # Return item as is for other types\n",
    "\n",
    "# Create dictionaries with converted types\n",
    "makes = [convert_for_json(x) for x in df['make'].unique()]\n",
    "models_by_make = {convert_for_json(k): [convert_for_json(i) for i in v] for k, v in df.groupby('make')['model'].unique().to_dict().items()}\n",
    "combustible_by_make_model = {convert_for_json(k[0] + '_' + k[1]): [convert_for_json(i) for i in v] for k, v in df.groupby(['make', 'model'])['Combustible'].unique().to_dict().items()}\n",
    "\n",
    "cambio_by_make_model = {convert_for_json(k[0] + '_' + k[1]): [convert_for_json(i) for i in v] for k, v in df.groupby(['make', 'model'])['Cambio'].unique().to_dict().items()}\n",
    "plazas_by_make_model = {convert_for_json(k[0] + '_' + k[1]): [convert_for_json(i) for i in v] for k, v in df.groupby(['make', 'model'])['Plazas (plazas)'].unique().to_dict().items()}\n",
    "potencia_by_make_model = {convert_for_json(k[0] + '_' + k[1]): [convert_for_json(i) for i in v] for k, v in df.groupby(['make', 'model'])['Potencia (cv)'].unique().to_dict().items()}\n",
    "\n",
    "# Write to JSON files\n",
    "with open('makes.json', 'w') as f:\n",
    "    json.dump(makes, f)\n",
    "\n",
    "with open('models_by_make.json', 'w') as f:\n",
    "    json.dump(models_by_make, f)\n",
    "\n",
    "with open('cambio_by_make_model.json', 'w') as f:\n",
    "    json.dump(cambio_by_make_model, f)\n",
    "\n",
    "with open('combustible_by_make_model.json', 'w') as f:\n",
    "    json.dump(combustible_by_make_model, f)\n",
    "\n",
    "with open('plazas_by_make_model.json', 'w') as f:\n",
    "    json.dump(plazas_by_make_model, f)\n",
    "    \n",
    "with open('potencia_by_make_model.json', 'w') as f:\n",
    "    json.dump(potencia_by_make_model, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0b9a96e-1887-43c4-bc29-700cf22c1f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating JSON files for website dropdowns with more connections \n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('clean_cochesdotcom_detail_pages_2024-05-13.csv')\n",
    "\n",
    "# Helper function to convert data types for JSON serialization\n",
    "def convert_for_json(item):\n",
    "    if isinstance(item, np.ndarray):\n",
    "        return item.tolist()  # Convert numpy arrays to Python list\n",
    "    elif isinstance(item, (np.int64, np.int32)):\n",
    "        return int(item)  # Convert numpy integers to Python int\n",
    "    elif isinstance(item, (np.float64, np.float32)):\n",
    "        return float(item)  # Convert numpy floats to Python float\n",
    "    return item  # Return item as is for other types\n",
    "\n",
    "# Extract unique colors\n",
    "fuel = df['Combustible'].unique().tolist()\n",
    "fuel = [convert_for_json(color) for color in fuel]\n",
    "\n",
    "# Extract unique cambio\n",
    "transmission = df['Cambio'].unique().tolist()\n",
    "transmission = [convert_for_json(transmission) for color in transmission]\n",
    "\n",
    "# Write fuel to JSON file\n",
    "with open('fuel.json', 'w') as f:\n",
    "    json.dump(fuel, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bef1369d-cbd6-4140-a44e-791a386c64ca",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '0-100 km/h (s)_by_make_model_combustible_plazas_cambio_potencia.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 73\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;66;03m# Write each attributes dictionary to JSON file\u001b[39;00m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m attr, data \u001b[38;5;129;01min\u001b[39;00m attribute_dict\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m---> 73\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.json\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m     74\u001b[0m         json\u001b[38;5;241m.\u001b[39mdump(data, f)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:310\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    304\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    305\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    306\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    307\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    308\u001b[0m     )\n\u001b[1;32m--> 310\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '0-100 km/h (s)_by_make_model_combustible_plazas_cambio_potencia.json'"
     ]
    }
   ],
   "source": [
    "#creating JSON files for website dropdowns with more connections \n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('clean_cochesdotcom_detail_pages_2024-05-13.csv')\n",
    "\n",
    "# Helper function to convert data types for JSON serialization\n",
    "def convert_for_json(item):\n",
    "    if isinstance(item, np.ndarray):\n",
    "        return item.tolist()  # Convert numpy arrays to Python list\n",
    "    elif isinstance(item, (np.int64, np.int32)):\n",
    "        return int(item)  # Convert numpy integers to Python int\n",
    "    elif isinstance(item, (np.float64, np.float32)):\n",
    "        return float(item)  # Convert numpy floats to Python float\n",
    "    return item  # Return item as is for other types\n",
    "\n",
    "# Extract unique colors\n",
    "colors = df['Color'].unique().tolist()\n",
    "colors = [convert_for_json(color) for color in colors]\n",
    "\n",
    "# Create dictionaries with converted types\n",
    "makes = [convert_for_json(x) for x in df['make'].unique()]\n",
    "models_by_make = {convert_for_json(k): [convert_for_json(i) for i in v] for k, v in df.groupby('make')['model'].unique().to_dict().items()}\n",
    "combustible_by_make_model = {convert_for_json(k[0] + '_' + k[1]): [convert_for_json(i) for i in v] for k, v in df.groupby(['make', 'model'])['Combustible'].unique().to_dict().items()}\n",
    "\n",
    "# Group by make, model and Combustible to get unique Cambio and Plazas (plazas)\n",
    "cambio_by_make_model_combustible = {convert_for_json(k[0] + '_' + k[1] + '_' + k[2]): [convert_for_json(i) for i in v] for k, v in df.groupby(['make', 'model', 'Combustible'])['Cambio'].unique().to_dict().items()}\n",
    "plazas_by_make_model_combustible = {convert_for_json(k[0] + '_' + k[1] + '_' + k[2]): [convert_for_json(i) for i in v] for k, v in df.groupby(['make', 'model', 'Combustible'])['Plazas (plazas)'].unique().to_dict().items()}\n",
    "\n",
    "# Group by make, model, Combustible, Plazas (plazas), and Cambio to get unique Potencia (cv)\n",
    "potencia_by_make_model_combustible_plazas_cambio = {convert_for_json('_'.join(map(str, k))): [convert_for_json(i) for i in v] for k, v in df.groupby(['make', 'model', 'Combustible', 'Plazas (plazas)', 'Cambio'])['Potencia (cv)'].unique().to_dict().items()}\n",
    "\n",
    "# Group by required attributes and count occurrences\n",
    "grouping_cols = ['make', 'model', 'Combustible', 'Plazas (plazas)', 'Potencia (cv)', 'Cambio']\n",
    "attributes = df.columns.difference(['make', 'model', 'Combustible', 'Plazas (plazas)', 'Potencia (cv)', 'Cambio', 'cash', 'Kms', 'Año', 'Color'])\n",
    "attribute_dict = {}\n",
    "\n",
    "for attr in attributes:\n",
    "    group = df.groupby(grouping_cols)[attr].apply(lambda x: x.mode()[0] if not x.empty else np.nan)\n",
    "    count = df.groupby(grouping_cols)[attr].size()\n",
    "    combined = group.reset_index().merge(count.reset_index(), on=grouping_cols)\n",
    "    combined.columns = grouping_cols + [f'{attr}_by_make_model_combustible_plazas_cambio_potencia', 'count']\n",
    "    attribute_dict[f'{attr}_by_make_model_combustible_plazas_cambio_potencia'] = combined.set_index(grouping_cols).to_dict(orient='index')\n",
    "\n",
    "\n",
    "# Write to JSON files\n",
    "with open('makes.json', 'w') as f:\n",
    "    json.dump(makes, f)\n",
    "\n",
    "with open('models_by_make.json', 'w') as f:\n",
    "    json.dump(models_by_make, f)\n",
    "\n",
    "with open('cambio_by_make_model_combustible.json', 'w') as f:\n",
    "    json.dump(cambio_by_make_model_combustible, f)\n",
    "\n",
    "with open('combustible_by_make_model.json', 'w') as f:\n",
    "    json.dump(combustible_by_make_model, f)\n",
    "\n",
    "with open('plazas_by_make_model_combustible.json', 'w') as f:\n",
    "    json.dump(plazas_by_make_model_combustible, f)\n",
    "\n",
    "with open('potencia_by_make_model_combustible_plazas_cambio.json', 'w') as f:\n",
    "    json.dump(potencia_by_make_model_combustible_plazas_cambio, f)\n",
    "\n",
    "# Write colors to JSON file\n",
    "with open('colors.json', 'w') as f:\n",
    "    json.dump(colors, f)\n",
    "\n",
    "# Write each attributes dictionary to JSON file\n",
    "for attr, data in attribute_dict.items():\n",
    "    with open(f'{attr}.json', 'w') as f:\n",
    "        json.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "630f90c4-3517-40d5-89c0-8b23745aca33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('clean_cochesdotcom_detail_pages_2024-05-13.csv')\n",
    "\n",
    "# Helper function to convert data types for JSON serialization\n",
    "def convert_for_json(item):\n",
    "    if isinstance(item, np.ndarray):\n",
    "        return item.tolist()  # Convert numpy arrays to Python list\n",
    "    elif isinstance(item, (np.int64, np.int32)):\n",
    "        return int(item)  # Convert numpy integers to Python int\n",
    "    elif isinstance(item, (np.float64, np.float32)):\n",
    "        return float(item)  # Convert numpy floats to Python float\n",
    "    return item  # Return item as is for other types\n",
    "\n",
    "# Function to sanitize attribute names for file names\n",
    "def sanitize_filename(name):\n",
    "    return re.sub(r'[^\\w\\s-]', '', name).strip().replace(' ', '_').replace('-', '_')\n",
    "\n",
    "# Define the grouping columns\n",
    "grouping_cols = ['make', 'model', 'Combustible', 'Plazas (plazas)', 'Cambio', 'Potencia (cv)']\n",
    "attributes = df.columns.difference(['make', 'model', 'Combustible', 'Plazas (plazas)', 'Cambio', 'Potencia (cv)', 'cash', 'Kms', 'Año', 'color'])\n",
    "attribute_dict = {}\n",
    "\n",
    "for attr in attributes:\n",
    "    # Get the mode for each group\n",
    "    mode = df.groupby(grouping_cols)[attr].apply(lambda x: x.mode()[0] if not x.empty else np.nan).reset_index(name='mode')\n",
    "\n",
    "    # Prepare data for JSON serialization\n",
    "    mode['group_key'] = mode[grouping_cols].apply(lambda x: '_'.join(x.map(str)), axis=1)\n",
    "    mode_dict = dict(zip(mode['group_key'], mode['mode'].apply(convert_for_json)))\n",
    "\n",
    "    attribute_dict[sanitize_filename(f'{attr}_by_make_model_combustible_plazas_cambio_potencia')] = mode_dict\n",
    "\n",
    "# Write each attributes dictionary to JSON file\n",
    "for attr, data in attribute_dict.items():\n",
    "    with open(f'{attr}.json', 'w') as f:\n",
    "        json.dump(data, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf28bc93-6040-482e-9c36-9757446d6721",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
